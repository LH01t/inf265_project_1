{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the same results with train and train_manual_update\n",
    "- Write torch.manual_seed(42) at the beginning of your notebook.\n",
    "- Write torch.set_default_dtype(torch.double) at the beginning of your notebook to alleviate precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3\n",
    "datasets: training, validation and test. Take a subset of these datasets\n",
    "by keeping only 2 labels: cat and car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar(train_val_split=0.9, data_path='../data/', preprocessor=None):\n",
    "\n",
    "    # transformer to resize images to 16x16 pixels\n",
    "    if preprocessor is None:\n",
    "        preprocessor = transforms.Compose([\n",
    "            transforms.Resize(16),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "    \n",
    "    # load datasets\n",
    "    train_val_data = datasets.CIFAR10(data_path, train=True, download=True, transform=preprocessor)\n",
    "    test_data = datasets.CIFAR10(data_path, train=False, download=True, transform=preprocessor)\n",
    "    \n",
    "    # sizes of train and validation data\n",
    "    train_size = int(train_val_split * len(train_val_data))\n",
    "    val_size = len(train_val_data) - train_size\n",
    "\n",
    "    # split train_val_data into train and validation sets\n",
    "    train_data, val_data = random_split(train_val_data, [train_size, val_size])\n",
    "\n",
    "    # create subsets with only cat (0) and car (1)\n",
    "    label_map = {3: 0, 1: 1} \n",
    "\n",
    "    train = [(img, label_map[label]) for img, label in train_data if label in [1,3]]\n",
    "    val = [(img, label_map[label]) for img, label in val_data if label in [1,3]]\n",
    "    test = [(img, label_map[label]) for img, label in test_data if label in [1,3]]\n",
    "\n",
    "    # create dataloaders?\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # disable gradient tracking\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            total += labels.size(0)\n",
    "            # compare predicted with labels\n",
    "            correct += torch.eq(predicted, labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully\n",
    "connected layers) such that:\n",
    "    \n",
    "    - The input dimension is 768(= 16 ∗ 16 ∗ 3) and the output dimension is 2 (for the 2 classes).\n",
    "    - The hidden layers have respectively 128 and 32 hidden units.\n",
    "    - All activation functions are ReLU. The last layer has no activation function since the cross-entropy loss already includes a softmax activation\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(16*16*3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train(n_epochs, optimizer, model, loss_fn, train_loader) function that trains model for n_epochs epochs given an optimizer optimizer, a loss function loss_fn and a dataloader train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        train_losses.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    \n",
    "    return train_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a similar function train manual_update that has no optimizer parameter, but a learning rate lr parameter instead and that manually updates each trainable parameter of model using equation (2). Do not forget to zero out all gradients after each iteration. \n",
    "\n",
    "Train 2 instances of MyMLP, one using train and the other using train_manual_update (use the same parameter values for both models). Compare their respective training losses. To get exactly the same results with both functions, see section 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, label in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # update all parameters\n",
    "            for p in model.parameters():\n",
    "                \n",
    "                # compute new parameter value\n",
    "                p.data -=  (lr * p.grad)\n",
    "                #p.grad = 0\n",
    "\n",
    "            # zero out the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        train_losses.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update_L2(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, label in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # update all parameters\n",
    "            for p in model.parameters():\n",
    "                \n",
    "                # compute new parameter value\n",
    "                p.data -=  weight_decay * lr * p.grad\n",
    "                #p.grad = 0\n",
    "\n",
    "            # zero out the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        train_losses.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 (not finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update_L2_my(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, label in train_loader:\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # update all parameters\n",
    "            t = 1\n",
    "            for p in model.parameters():\n",
    "                \n",
    "                g_t = p.grad\n",
    "\n",
    "                # with weight decay\n",
    "                if weight_decay != 0:\n",
    "                    g_t += weight_decay * p.data #p.data here or grad? data in SGD notes?\n",
    "\n",
    "                # with momentum\n",
    "                if momentum_coeff != 0: \n",
    "                    if t > 1:\n",
    "                        print(b_t.shape, g_t.shape)\n",
    "                        b_t = (momentum_coeff * b_t) + g_t\n",
    "                    else:\n",
    "                        b_t = g_t\n",
    "\n",
    "                    g_t = b_t\n",
    "\n",
    "                # compute new parameter value\n",
    "                p.data -=  lr * g_t\n",
    "\n",
    "                t += 1\n",
    "\n",
    "            # zero out the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        train_losses.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "data_train, data_val, data_test = load_cifar()\n",
    "\n",
    "# create an instance of MyNet\n",
    "torch.manual_seed(42)\n",
    "model_1 = MyNet()\n",
    "torch.manual_seed(42)\n",
    "model_2 = MyNet()\n",
    "\n",
    "# optimizer, loss function, dataloader\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:39:43.732543  |  Epoch 1  |  Training loss 0.435\n",
      "12:39:44.287683  |  Epoch 10  |  Training loss 0.349\n",
      "[0.435492932385567, 0.42678182860324104, 0.41777411167456774, 0.40836909292769846, 0.39845052451651397, 0.38817815523412974, 0.3778513767732199, 0.36760161499761973, 0.3576546298809369, 0.3485385585318173]\n"
     ]
    }
   ],
   "source": [
    "train_losses = train(\n",
    "    n_epochs = 10,\n",
    "    optimizer= optimizer, \n",
    "    model = model_1, \n",
    "    loss_fn = loss_fn, \n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:39:44.985468  |  Epoch 1  |  Training loss 0.435\n",
      "12:39:45.579355  |  Epoch 10  |  Training loss 0.349\n",
      "[0.43549293238556697, 0.42678182860324104, 0.41777411167456774, 0.4083690929276986, 0.39845052451651397, 0.38817815523412974, 0.3778513767732198, 0.36760161499761973, 0.3576546298809369, 0.3485385585318173]\n"
     ]
    }
   ],
   "source": [
    "train_manual_losses = train_manual_update(\n",
    "    n_epochs = 10,\n",
    "    model = model_2, \n",
    "    loss_fn = loss_fn, \n",
    "    train_loader = train_loader\n",
    ")\n",
    "print(train_manual_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train_losses == train_manual_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:44:08.553518  |  Epoch 1  |  Training loss 0.149\n",
      "08:44:09.197209  |  Epoch 10  |  Training loss 0.135\n",
      "[0.1486143608688836, 0.14691703552031246, 0.1450711569680034, 0.1434314581946205, 0.1412237691715824, 0.13970836757279684, 0.13839612823876332, 0.13731512321168213, 0.1356373076203175, 0.13499526262928313]\n"
     ]
    }
   ],
   "source": [
    "train_manual_losses_L2 = train_manual_update_L2(\n",
    "    n_epochs = 10,\n",
    "    model = model_2, \n",
    "    loss_fn = loss_fn, \n",
    "    train_loader = train_loader, \n",
    "    weight_decay=0.9 # what to put to?\n",
    ")\n",
    "print(train_manual_losses_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 768]) torch.Size([128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_manual_losses_L2_my \u001b[38;5;241m=\u001b[39m train_manual_update_L2_my(\n\u001b[1;32m      2\u001b[0m     n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_2, \n\u001b[1;32m      4\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m loss_fn, \n\u001b[1;32m      5\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m train_loader, \n\u001b[1;32m      6\u001b[0m     momentum_coeff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;66;03m# what to put to?\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_manual_losses_L2_my)\n",
      "Cell \u001b[0;32mIn[59], line 30\u001b[0m, in \u001b[0;36mtrain_manual_update_L2_my\u001b[0;34m(n_epochs, model, loss_fn, train_loader, lr, momentum_coeff, weight_decay)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(b_t\u001b[38;5;241m.\u001b[39mshape, g_t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 30\u001b[0m     b_t \u001b[38;5;241m=\u001b[39m (momentum_coeff \u001b[38;5;241m*\u001b[39m b_t) \u001b[38;5;241m+\u001b[39m g_t\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     b_t \u001b[38;5;241m=\u001b[39m g_t\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "train_manual_losses_L2_my = train_manual_update_L2_my(\n",
    "    n_epochs = 10,\n",
    "    model = model_2, \n",
    "    loss_fn = loss_fn, \n",
    "    train_loader = train_loader, \n",
    "    momentum_coeff=0.1,\n",
    "    weight_decay=0.9 # what to put to?\n",
    ")\n",
    "print(train_manual_losses_L2_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
